{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data = pd.read_csv('TrainDataV.NoNormalize3.csv', delimiter = ',').reset_index(drop=True)\n",
    "data = pd.read_csv('normalizedtrainfinal.csv', delimiter = ',').reset_index(drop=True)\n",
    "# train = pd.read_csv('train.csv', delimiter = ',').reset_index(drop=True)normalizedtrainfinal\n",
    "# test = pd.read_csv('test.csv', delimiter = ',').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ans = pd.read_csv('normalizedtestfinal.csv', delimiter = ',').reset_index(drop=True)\n",
    "data_ans = data_ans.sort_values(by=['Right_ip_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3938, 327)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ans = data_ans.drop(['Right_ip_id','ip_id'], axis=1).fillna(0)\n",
    "x_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5088, 327) (5088, 1) (1273, 327) (1273, 1)\n"
     ]
    }
   ],
   "source": [
    "####### no train test ###########\n",
    "data1 = data.drop(['Targat'], axis=1).fillna(0)\n",
    "# df.sort_values(by=['col1'])\n",
    "# data1 = data[['SmartTile_Name','SmartTile_Name2','SmartTile_Name3','SmartTile_Name4','SmartTile_Name5','SmartTile_Name6','SmartTile_Name7',\n",
    "# 'SmartTile_Name8','SmartTile_Name9','SmartTile_Name9_2','SmartTile_Name9_3','SmartTile_Name9_4','SmartTile_Name9_5','SmartTile_Name9_6',\n",
    "# 'SmartTile_Name9_7','SmartTile_Name9_8','SmartTile_Name9_9','SmartTile_Name9_10','New_gnd_cd','New_mar_st_cd',\n",
    "# 'New_ctf_tp_cd','New_ocp_cd','New_IsBangkok']]\n",
    "# ['Targat','TotalSum_amt','ProvinceAmt','ChildPerBalance','SalaPerChild','BalanPerSala',\n",
    "#                   'AccAgePerAge','AgeWhenOpenAcc','Age','YearAccStart','crn_bal','cis_income','no_of_dpnd_chl','brth_yr',\n",
    "#                   'AccAge','TotalMedian_amt','TotalSumCC','TotalMedCC','TotalFQcc']\n",
    "\n",
    "y = data[['Targat']].values\n",
    "x = data1\n",
    "x_train_, x_test_, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "###### x,y train test #########\n",
    "# x_train_ = train.drop(['Target'], axis=1).fillna(0)\n",
    "# y_train = train[['Target']].values\n",
    "\n",
    "# x_test_ = test.drop(['Target'], axis=1).fillna(0)\n",
    "# y_test = test[['Target']].values\n",
    "\n",
    "\n",
    "####### for concat ########\n",
    "x1 = pd.concat([x, x_ans])\n",
    "\n",
    "\n",
    "print(x_train_.shape,y_train_.shape,x_test_.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 327)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy = data.copy()\n",
    "# data_copy['test'] = 0\n",
    "# data_copy[data_copy['TotalSum_amt'] >= 350000][['Targat']].values.sum()\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x000001679A938400>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 482, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n",
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5088, 327) (5088, 1) (1273, 327) (1273, 1) (6361, 327)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "####### for transform index category #######\n",
    "############################################\n",
    "import Encoder as en\n",
    "import numpy as np\n",
    "\n",
    "mcle = en.MultiColumnLabelEncoder(columns=np.array(['SmartTile_Name','SmartTile_Name2','SmartTile_Name3','SmartTile_Name4','SmartTile_Name5','SmartTile_Name6','SmartTile_Name7',\n",
    "'SmartTile_Name8','SmartTile_Name9','SmartTile_Name9_2','SmartTile_Name9_3','SmartTile_Name9_4','SmartTile_Name9_5','SmartTile_Name9_6',\n",
    "'SmartTile_Name9_7','SmartTile_Name9_8','SmartTile_Name9_9','SmartTile_Name9_10','New_gnd_cd','New_mar_st_cd',\n",
    "'New_ctf_tp_cd','New_ocp_cd','New_IsBangkok','New_ProvinceAmt_Indicator','New_SAQ42017Ratio_Indicator','New_CCfreqGrowth_Indicator']))\n",
    "mcle.fit(x1)\n",
    "mcle.transform(x_test_)\n",
    "mcle.transform(x_train_)\n",
    "mcle.transform(x_ans)\n",
    "mcle.transform(x)\n",
    "x_train = x_train_.values\n",
    "x_test = x_test_.values\n",
    "x_ans = x_ans.values\n",
    "x = x.values\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5088, 424) (1273, 424)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "########## for encode one hot key ##########\n",
    "############################################\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "x = data1.to_dict('records')\n",
    "x_train = x_train_.to_dict('records')\n",
    "x_test = x_test_.to_dict('records')\n",
    "\n",
    "vec = DictVectorizer()\n",
    "x = vec.fit_transform(x).toarray()\n",
    "x_train = vec.transform(x_train).toarray()\n",
    "x_test = vec.transform(x_test).toarray()\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brth_yr  no_of_dpnd_chl  cis_income  crn_bal  YearAccStart  Age  AccAge  \\\n",
      "896      1976               0       42337      500          2007   42      11   \n",
      "4803     1977               1       40767   213609          2014   41       4   \n",
      "251      1978               0       23740  1537068          2015   40       3   \n",
      "1209     1976               0       45066   707949          2010   42       8   \n",
      "5955     1977               0       43107   428258          2009   41       9   \n",
      "5026     1975               0       40713   252342          2013   43       5   \n",
      "3206     1972               0       42333   738940          2008   46      10   \n",
      "3337     1979               0       43985   616622          2007   39      11   \n",
      "1128     1975               0       44307      500          2010   43       8   \n",
      "4331     1977               0       45951      500          2010   41       8   \n",
      "3098     1977               0       44963   341985          2010   41       8   \n",
      "57       1997               2       58525      500          2011   21       7   \n",
      "4280     1964               0       26283      500          1996   54      22   \n",
      "3024     1985               0       52387   351845          2010   33       8   \n",
      "6318     1980               1       37266   351339          2010   38       8   \n",
      "292      1983               0       40079   101907          2006   35      12   \n",
      "1142     1981               0       38268   311586          2006   37      12   \n",
      "2669     1978               0       44897      500          2009   40       9   \n",
      "1483     1977               0       45484   504935          2012   41       6   \n",
      "1170     1982               0       43862      500          2008   36      10   \n",
      "5823     1985               2       44950   890327          2011   33       7   \n",
      "5304     1982               0       48065      500          2012   36       6   \n",
      "5410     1983               0       45975      500          2006   35      12   \n",
      "3547     1983               0       45663   546024          2008   35      10   \n",
      "2484     1979               1       49152   368031          2010   39       8   \n",
      "1086     1976               1       45358      500          2008   42      10   \n",
      "3360     1979               0       48947      500          2008   39      10   \n",
      "1172     1973               0       44281   813132          2008   45      10   \n",
      "3107     1981               0       45508   997090          2013   37       5   \n",
      "3220     1976               0       41015      500          2005   42      13   \n",
      "...       ...             ...         ...      ...           ...  ...     ...   \n",
      "2904     1976               0       37884  1500597          2011   42       7   \n",
      "4843     1976               0       36745   153550          2016   42       2   \n",
      "4117     1977               0       35307   826783          2008   41      10   \n",
      "3385     1985               0       43561      500          2008   33      10   \n",
      "4555     1977               0       40303   941154          2012   41       6   \n",
      "1184     1980               0       41369   768100          2008   38      10   \n",
      "5051     1978               1       42775   528590          2010   40       8   \n",
      "5311     1989               0       39571      500          2009   29       9   \n",
      "2433     1982               0       38714      500          2006   36      12   \n",
      "5611     1977               0       38935      500          2011   41       7   \n",
      "2391     1979               0       43126   503133          2009   39       9   \n",
      "769      1983               0       41457      500          2011   35       7   \n",
      "1685     1972               0       45705      500          2011   46       7   \n",
      "130      2005               0       77486      500          2002   13      16   \n",
      "2919     1978               0       42581   752023          2017   40       1   \n",
      "3171     1987               0       40407      500          2012   31       6   \n",
      "3444     1987               0       45242   520414          2010   31       8   \n",
      "6231     1982               3       45885   431226          2007   36      11   \n",
      "5578     1967               0       43421      500          2008   51      10   \n",
      "4426     1982               1       45768  1352285          2015   36       3   \n",
      "5334     1981               1       44144   381446          2011   37       7   \n",
      "466      1978               0       35567  1142501          2006   40      12   \n",
      "6265     1976               0       36846    73147          2009   42       9   \n",
      "5734     1984               1       44772      500          2009   34       9   \n",
      "3092     1973               0       43486   282115          2015   45       3   \n",
      "3772     1979               1       42025      500          2011   39       7   \n",
      "5191     1975               1       44631   206129          2007   43      11   \n",
      "5226     1980               0       43893      500          2004   38      14   \n",
      "5390     1983               1       41618      500          2011   35       7   \n",
      "860      1973               0       46830  1280772          2011   45       7   \n",
      "\n",
      "      AgeWhenOpenAcc  AccAgePerAge  BalanPerSala      ...        \\\n",
      "896               31      0.261905      0.011833      ...         \n",
      "4803              37      0.097561      5.239649      ...         \n",
      "251               37      0.075000     64.743229      ...         \n",
      "1209              34      0.190476     15.708834      ...         \n",
      "5955              32      0.219512      9.934560      ...         \n",
      "5026              38      0.116279      6.197942      ...         \n",
      "3206              36      0.217391     17.455024      ...         \n",
      "3337              28      0.282051     14.018620      ...         \n",
      "1128              35      0.186047      0.011307      ...         \n",
      "4331              33      0.195122      0.010903      ...         \n",
      "3098              33      0.195122      7.605774      ...         \n",
      "57                14      0.333333      0.008560      ...         \n",
      "4280              32      0.407407      0.019061      ...         \n",
      "3024              25      0.242424      6.716156      ...         \n",
      "6318              30      0.210526      9.427644      ...         \n",
      "292               23      0.342857      2.542615      ...         \n",
      "1142              25      0.324324      8.142021      ...         \n",
      "2669              31      0.225000      0.011159      ...         \n",
      "1483              35      0.146341     11.101154      ...         \n",
      "1170              26      0.277778      0.011422      ...         \n",
      "5823              26      0.212121     19.806634      ...         \n",
      "5304              30      0.166667      0.010423      ...         \n",
      "5410              23      0.342857      0.010897      ...         \n",
      "3547              25      0.285714     11.957450      ...         \n",
      "2484              31      0.205128      7.487478      ...         \n",
      "1086              32      0.238095      0.011045      ...         \n",
      "3360              29      0.256410      0.010235      ...         \n",
      "1172              35      0.222222     18.362608      ...         \n",
      "3107              32      0.135135     21.909754      ...         \n",
      "3220              29      0.309524      0.012215      ...         \n",
      "...              ...           ...           ...      ...         \n",
      "2904              35      0.166667     39.609291      ...         \n",
      "4843              40      0.047619      4.178713      ...         \n",
      "4117              31      0.243902     23.416336      ...         \n",
      "3385              23      0.303030      0.011501      ...         \n",
      "4555              35      0.146341     23.351404      ...         \n",
      "1184              28      0.263158     18.566618      ...         \n",
      "5051              32      0.200000     12.357186      ...         \n",
      "5311              20      0.310345      0.012660      ...         \n",
      "2433              24      0.333333      0.012941      ...         \n",
      "5611              34      0.170732      0.012867      ...         \n",
      "2391              30      0.230769     11.666334      ...         \n",
      "769               28      0.200000      0.012085      ...         \n",
      "1685              39      0.152174      0.010961      ...         \n",
      "130               -3      1.230769      0.006466      ...         \n",
      "2919              39      0.025000     17.660608      ...         \n",
      "3171              25      0.193548      0.012399      ...         \n",
      "3444              23      0.258065     11.502663      ...         \n",
      "6231              25      0.305556      9.397790      ...         \n",
      "5578              41      0.196078      0.011538      ...         \n",
      "4426              33      0.083333     29.545894      ...         \n",
      "5334              30      0.189189      8.640775      ...         \n",
      "466               28      0.300000     32.121626      ...         \n",
      "6265              33      0.214286      1.985182      ...         \n",
      "5734              25      0.264706      0.011190      ...         \n",
      "3092              42      0.066667      6.487364      ...         \n",
      "3772              32      0.179487      0.011921      ...         \n",
      "5191              32      0.255814      4.618435      ...         \n",
      "5226              24      0.368421      0.011414      ...         \n",
      "5390              28      0.200000      0.012038      ...         \n",
      "860               38      0.155556     27.348829      ...         \n",
      "\n",
      "      SmartTile_Name9_6  SmartTile_Name9_7  SmartTile_Name9_8  \\\n",
      "896                   0                  1                  0   \n",
      "4803                  1                  1                  0   \n",
      "251                   1                  1                  0   \n",
      "1209                  1                  1                  0   \n",
      "5955                  1                  0                  0   \n",
      "5026                  3                  1                  0   \n",
      "3206                  0                  1                  0   \n",
      "3337                  0                  1                  1   \n",
      "1128                  2                  1                  1   \n",
      "4331                  1                  1                  0   \n",
      "3098                  1                  1                  0   \n",
      "57                    1                  1                  0   \n",
      "4280                  0                  1                  0   \n",
      "3024                  0                  1                  0   \n",
      "6318                  1                  1                  0   \n",
      "292                   0                  1                  0   \n",
      "1142                  1                  1                  0   \n",
      "2669                  1                  1                  1   \n",
      "1483                  1                  0                  1   \n",
      "1170                  0                  1                  0   \n",
      "5823                  1                  1                  0   \n",
      "5304                  0                  1                  0   \n",
      "5410                  2                  1                  0   \n",
      "3547                  1                  1                  1   \n",
      "2484                  2                  1                  0   \n",
      "1086                  1                  1                  0   \n",
      "3360                  1                  1                  1   \n",
      "1172                  1                  1                  1   \n",
      "3107                  1                  1                  0   \n",
      "3220                  0                  0                  1   \n",
      "...                 ...                ...                ...   \n",
      "2904                  0                  1                  0   \n",
      "4843                  0                  0                  0   \n",
      "4117                  1                  1                  1   \n",
      "3385                  1                  1                  0   \n",
      "4555                  2                  1                  0   \n",
      "1184                  0                  1                  1   \n",
      "5051                  0                  0                  1   \n",
      "5311                  1                  1                  0   \n",
      "2433                  2                  0                  0   \n",
      "5611                  1                  1                  0   \n",
      "2391                  0                  1                  1   \n",
      "769                   0                  1                  0   \n",
      "1685                  1                  1                  0   \n",
      "130                   1                  1                  0   \n",
      "2919                  1                  1                  1   \n",
      "3171                  1                  1                  0   \n",
      "3444                  0                  0                  0   \n",
      "6231                  2                  1                  0   \n",
      "5578                  1                  1                  1   \n",
      "4426                  1                  1                  0   \n",
      "5334                  1                  1                  0   \n",
      "466                   2                  1                  1   \n",
      "6265                  1                  1                  1   \n",
      "5734                  2                  1                  1   \n",
      "3092                  1                  1                  1   \n",
      "3772                  1                  1                  0   \n",
      "5191                  1                  1                  0   \n",
      "5226                  1                  1                  0   \n",
      "5390                  2                  1                  0   \n",
      "860                   1                  1                  1   \n",
      "\n",
      "      SmartTile_Name9_9  SmartTile_Name9_10  New_gnd_cd  New_mar_st_cd  \\\n",
      "896                   1                   0           1              3   \n",
      "4803                  0                   2           1              2   \n",
      "251                   1                   5           0              0   \n",
      "1209                  1                   0           1              1   \n",
      "5955                  1                   6           0              1   \n",
      "5026                  1                   2           0              3   \n",
      "3206                  1                   1           1              2   \n",
      "3337                  1                   1           0              3   \n",
      "1128                  1                   0           0              2   \n",
      "4331                  1                   1           1              2   \n",
      "3098                  1                   1           1              0   \n",
      "57                    0                   5           0              3   \n",
      "4280                  0                   1           1              0   \n",
      "3024                  1                   1           1              2   \n",
      "6318                  1                   4           1              3   \n",
      "292                   4                   0           1              2   \n",
      "1142                  0                   0           0              2   \n",
      "2669                  1                   1           1              1   \n",
      "1483                  1                   0           0              0   \n",
      "1170                  1                   0           0              0   \n",
      "5823                  0                   6           1              1   \n",
      "5304                  1                   2           1              0   \n",
      "5410                  1                   2           1              3   \n",
      "3547                  1                   1           0              2   \n",
      "2484                  1                   1           0              2   \n",
      "1086                  1                   0           0              0   \n",
      "3360                  1                   1           0              1   \n",
      "1172                  0                   0           0              0   \n",
      "3107                  1                   1           1              0   \n",
      "3220                  1                   1           1              0   \n",
      "...                 ...                 ...         ...            ...   \n",
      "2904                  1                   1           0              1   \n",
      "4843                  0                   2           1              0   \n",
      "4117                  1                   1           1              0   \n",
      "3385                  1                   1           0              2   \n",
      "4555                  0                   2           1              3   \n",
      "1184                  1                   0           0              1   \n",
      "5051                  0                   2           1              3   \n",
      "5311                  0                   2           1              2   \n",
      "2433                  1                   1           1              1   \n",
      "5611                  1                   2           0              1   \n",
      "2391                  1                   1           1              2   \n",
      "769                   0                   0           0              1   \n",
      "1685                  1                   0           0              2   \n",
      "130                   1                   5           1              2   \n",
      "2919                  0                   1           0              0   \n",
      "3171                  1                   1           0              3   \n",
      "3444                  1                   1           1              2   \n",
      "6231                  1                   4           0              2   \n",
      "5578                  1                   2           1              1   \n",
      "4426                  1                   1           0              2   \n",
      "5334                  1                   2           1              0   \n",
      "466                   1                   0           0              0   \n",
      "6265                  1                   4           1              2   \n",
      "5734                  1                   6           1              2   \n",
      "3092                  1                   1           0              1   \n",
      "3772                  0                   1           1              3   \n",
      "5191                  1                   2           1              1   \n",
      "5226                  1                   2           0              3   \n",
      "5390                  1                   2           0              1   \n",
      "860                   0                   0           1              2   \n",
      "\n",
      "      New_ctf_tp_cd  New_ocp_cd  New_IsBangkok  \n",
      "896               2          10              0  \n",
      "4803              1           2              0  \n",
      "251               0           0              1  \n",
      "1209              0           3              0  \n",
      "5955              1           9              1  \n",
      "5026              4           1              0  \n",
      "3206              0           2              1  \n",
      "3337              5           4              0  \n",
      "1128              5           3              1  \n",
      "4331              1           5              1  \n",
      "3098              4           8              1  \n",
      "57                4           9              1  \n",
      "4280              3           2              1  \n",
      "3024              2           4              1  \n",
      "6318              0           5              1  \n",
      "292               1          10              0  \n",
      "1142              5           2              1  \n",
      "2669              1           9              1  \n",
      "1483              2           8              1  \n",
      "1170              2           4              0  \n",
      "5823              0           0              1  \n",
      "5304              1           0              0  \n",
      "5410              2           1              1  \n",
      "3547              1           1              1  \n",
      "2484              1           5              1  \n",
      "1086              5           1              0  \n",
      "3360              1           7              1  \n",
      "1172              4           0              0  \n",
      "3107              1           3              0  \n",
      "3220              1           2              0  \n",
      "...             ...         ...            ...  \n",
      "2904              1           3              1  \n",
      "4843              0           5              0  \n",
      "4117              0           6              1  \n",
      "3385              2          10              0  \n",
      "4555              4          10              1  \n",
      "1184              2           2              1  \n",
      "5051              2           2              1  \n",
      "5311              2           7              0  \n",
      "2433              1           5              0  \n",
      "5611              1           7              1  \n",
      "2391              4           3              0  \n",
      "769               1           5              1  \n",
      "1685              5          10              1  \n",
      "130               2           7              1  \n",
      "2919              0           5              0  \n",
      "3171              1           7              0  \n",
      "3444              0          10              1  \n",
      "6231              4           3              1  \n",
      "5578              0           2              0  \n",
      "4426              1           9              0  \n",
      "5334              0           7              0  \n",
      "466               1           0              1  \n",
      "6265              0           2              1  \n",
      "5734              5          10              1  \n",
      "3092              1          10              0  \n",
      "3772              1           7              1  \n",
      "5191              5           3              1  \n",
      "5226              5          10              1  \n",
      "5390              0          10              0  \n",
      "860               3           8              0  \n",
      "\n",
      "[5088 rows x 326 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16093, 172) (16093,)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################ imbalance #################\n",
    "############################################\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "sm = ADASYN(random_state=42)\n",
    "x_sm, y_sm = sm.fit_sample(x_train, y_train)\n",
    "\n",
    "\n",
    "print(x_sm.shape,y_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6361, 25) (6361, 1)\n",
      "(5088, 25) (5088, 1) (1273, 25) (1273, 1)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################### PCA ####################\n",
    "############################################\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 25\n",
    "pca = PCA(n_components=n_components, svd_solver='full',random_state=42)\n",
    "pca.fit(x)\n",
    "x = pca.transform(x)\n",
    "x_sm = pca.transform(x_sm)\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "\n",
    "print(x.shape,y.shape)\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############## not imbalance ###############\n",
    "############################################\n",
    "x_sm = x_train \n",
    "y_sm = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5088, 327) (4000, 327) (4000, 1) (1088, 327) (1088, 1)\n",
      "(5088, 1)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "########## optional for val data ###########\n",
    "############################################\n",
    "x_sm0 = x_sm[:4000]\n",
    "x_val0 = x_sm[4000:]\n",
    "y_sm0 = y_sm[:4000]\n",
    "y_val0 = y_sm[4000:]\n",
    "print(x_sm.shape,x_sm0.shape,y_sm0.shape,x_val0.shape,y_val0.shape)\n",
    "print(y_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5088, 327) (4000, 327) (4000, 1) (1088, 327) (1088, 1) (1273, 327) (6361, 327)\n"
     ]
    }
   ],
   "source": [
    "print(x_sm.shape,x_sm0.shape,y_sm0.shape,x_val0.shape,y_val0.shape,x_test.shape,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############### Xgboost ####################\n",
    "############################################\n",
    "import xgboost as xgb\n",
    "dsm = xgb.DMatrix(x_sm0, label=y_sm0)\n",
    "dv0 = xgb.DMatrix(x_val0, label=y_val0)\n",
    "dt = xgb.DMatrix(x_test, label=y_test)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "evallist = [(dv0, 'eval'), (dsm, 'train')]\n",
    "                  \n",
    "# dsm = xgb.DMatrix(x_sm, label=y_sm)                  \n",
    "# evallist = [(dt, 'eval'), (dsm, 'train')]\n",
    "dx = xgb.DMatrix(x, label=y)\n",
    "num_round = 10000\n",
    "# binary:logistic\n",
    "param = {'objective': 'binary:logistic',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    " 'learning_rate': 0.2249426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 18,\n",
    " 'objective': 'binary:logistic',\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'auc',\n",
    " 'lambda': 3,\n",
    " 'alpha': 16,\n",
    " 'rate_drop':0.950292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921}\n",
    " \n",
    "evals_result = {}\n",
    "\n",
    "# {'alpha': 17,\n",
    "#  'colsample_bytree': 0.9866641258270036,\n",
    "#  'gamma': 3.8835260600913024,\n",
    "#  'lambda': 4,\n",
    "#  'learning_rate': 0.23910223888617654,\n",
    "#  'max_depth': 14,\n",
    "#  'min_child_weight': 162.51127167841472,\n",
    "#  'n_estimators': 18,\n",
    "#  'objective': 'binary:logistic',\n",
    "#  'rate_drop': 0.9502928648791279,\n",
    "#  'scale_pos_weight': 1,\n",
    "#  'seed': 42,\n",
    "#  'subsample': 0.8944732072319205}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[0]\teval-auc:1\ttrain-auc:1\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 10 rounds.\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[1]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[2]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[3]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[4]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[5]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[6]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[7]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[8]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[9]\teval-auc:1\ttrain-auc:1\n",
      "[19:49:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[10]\teval-auc:1\ttrain-auc:1\n",
      "Stopping. Best iteration:\n",
      "[0]\teval-auc:1\ttrain-auc:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### train ####\n",
    "bst = xgb.train(param, dx, num_round, evallist, evals_result=evals_result,early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "#### evaluate ###\n",
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score , roc_curve , auc , classification_report\n",
    "threshold = 0.3907732\n",
    "treee = bst.best_ntree_limit\n",
    "# treee = 27\n",
    "y_pre = np.array(bst.predict(dt, ntree_limit=treee))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pre, pos_label=1)\n",
    "roc1_1  = auc(fpr, tpr)\n",
    "\n",
    "y_pre_1 = np.array(bst.predict(dv0, ntree_limit=treee))\n",
    "fpr, tpr, thresholds = roc_curve(y_val0, y_pre_1, pos_label=1)\n",
    "roc1_11  = auc(fpr, tpr)\n",
    "\n",
    "y_pre_2 = np.array(bst.predict(dtrain, ntree_limit=treee))\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pre_2, pos_label=1)\n",
    "roc1_2  = auc(fpr, tpr)\n",
    "print(roc1_1,roc1_11,roc1_2)\n",
    "# print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = np.array(bst.predict(dt, ntree_limit=treee))\n",
    "y_pre  = y_pre > threshold\n",
    "y_pre = y_pre.astype(int) \n",
    "f1_1  = f1_score(y_test, y_pre, average='binary')\n",
    "f1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39025742, 0.39025742, 0.39025742, ..., 0.39025742, 0.39025742,\n",
       "       0.39025742], dtype=float32)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bst.predict(dt, ntree_limit=treee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-486-2c457af9b464>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-486-2c457af9b464>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    1 = 1\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": [
    "a = bst.predict(dt, ntree_limit=treee)\n",
    "np.argwhere(a>0.4)\n",
    "np.argwhere(y_test>0.4)\n",
    "1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best params ###\n",
    "\n",
    "# param = {'objective': 'binary:logistic',\n",
    "#  'colsample_bytree': 0.9683760122352089,\n",
    "#  'gamma': 0.7790711924812199,\n",
    "#  'learning_rate': 0.2249426498504554,\n",
    "#  'max_depth': 20,\n",
    "#  'min_child_weight': 10.95324500379702,\n",
    "#  'n_estimators': 150,\n",
    "#  'objective': 'binary:logistic',\n",
    "#  'scale_pos_weight': 1,\n",
    "#  'seed': 42,\n",
    "#  'eval_metric': ['auc'],\n",
    "#  'lambda': 2,\n",
    "#  'alpha': 15,\n",
    "# #  'rate_drop':0.5,\n",
    "#  'tree_method':'exact',\n",
    "#  'normalize_type':'forest',\n",
    "#  'subsample': 0.9035691355661921}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x16799854240>"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### feature important ######\n",
    "xgb.plot_importance(bst,max_num_features =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "######## scikit-learn multi-models #########\n",
    "############################################\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "import catboost as cb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "n_jobs = -1\n",
    "n_iter = 100\n",
    "n_iter_nt = 3\n",
    "n_components = 25\n",
    "cv = 5\n",
    "seed=42\n",
    "n_features= x_sm.shape[1]\n",
    "# n_features= n_components\n",
    "is_pca = False\n",
    "scoring = 'roc_auc'\n",
    "# is_pca = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### models ################ param\n",
    "\n",
    "# estimator = XGBClassifier(nthreads=-1,tree_method='exact')\n",
    "param = {'objective': 'binary:logistic',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma': 0.7790711924812199,\n",
    " 'learning_rate': 0.2249426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 150,\n",
    " 'objective': 'binary:logistic',\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': ['auc'],\n",
    " 'lambda': 2,\n",
    " 'alpha': 15,\n",
    " 'rate_drop':0.1,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921}\n",
    "\n",
    "estimator = XGBClassifier(**param)\n",
    "objective = 'binary:logistic'\n",
    "\n",
    "# Parameter for XGBoost\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 40),\n",
    "    \"max_depth\": st.randint(3, 30),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": st.beta(10, 1),\n",
    "    \"subsample\": st.beta(10, 1),\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'objective': [objective],\n",
    "    'scale_pos_weight': st.randint(0, 2),\n",
    "    \"min_child_weight\": st.expon(0, 50),\n",
    "    'lambda': st.randint(1, 20),\n",
    "    'alpha': st.randint(0, 20),\n",
    "    'rate_drop':st.uniform(0, 1),\n",
    "    \"seed\": [seed],\n",
    "}\n",
    "\n",
    "xgb = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', xgb)]\n",
    "    xgb = Pipeline(estimators)\n",
    "    xgb = GridSearchCV(xgb, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    \n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "# Parameter for LogisticRegression\n",
    "params = {\n",
    "    \"penalty\": ['l1','l2'],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"tol\": [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "    \"max_iter\": st.randint(50, 100),\n",
    "    'random_state': [seed],\n",
    "} \n",
    "log = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter , scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', log)]\n",
    "    log = Pipeline(estimators)\n",
    "    log = GridSearchCV(log, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = KNeighborsClassifier()\n",
    "# Parameter for KNeighborsClassifier\n",
    "params = {\n",
    "    \"n_neighbors\": st.randint(2, 50),\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"algorithm\": ['ball_tree', 'kd_tree', 'brute'],\n",
    "    \"leaf_size\": st.randint(10, 30),\n",
    "    \"p\": st.randint(1, 2),\n",
    "}\n",
    "knn = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter_nt , scoring = scoring)\n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', knn)]\n",
    "    knn = Pipeline(estimators)\n",
    "    \n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "# Parameter for RandomForestClassifier\n",
    "params = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "rnf = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', rnf)]\n",
    "    rnf = Pipeline(estimators)\n",
    "    rnf = GridSearchCV(rnf, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    \n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = ExtraTreesClassifier()\n",
    "# \n",
    "# Parameter for ExtraTreesClassifier\n",
    "params = {\n",
    "    \"n_estimators\": st.randint(5, 50),\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    \"bootstrap\": [True],\n",
    "    \"oob_score\": [True],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "ext = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', ext)]\n",
    "    ext = Pipeline(estimators)\n",
    "    ext = GridSearchCV(ext, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = AdaBoostClassifier()\n",
    "# Parameter for AdaBoost\n",
    "params = { \n",
    "    'n_estimators':st.randint(10, 100), \n",
    "    'learning_rate':st.beta(10, 1), \n",
    "    'algorithm':['SAMME', 'SAMME.R'],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "ada = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', ada)]\n",
    "    ada = Pipeline(estimators)\n",
    "    ada = GridSearchCV(ada, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    \n",
    "\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "estimator = SVC()\n",
    "# Parameter for SVC\n",
    "params = {  \n",
    "    'C':[0.001, 0.01, 0.1, 1, 10], \n",
    "    'degree': st.randint(1, 10),\n",
    "    'shrinking': [True, False],\n",
    "    'probability': [True],\n",
    "    'tol': [1e-3],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "svc = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter_nt, scoring = scoring)\n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', svc)]\n",
    "    svc = Pipeline(estimators)\n",
    "#     svc = GridSearchCV(svc, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "# Parameter for LGBMClassifier\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "estimator = LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          n_jobs = 3, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': [0.005],\n",
    "    'n_estimators': [40,30],\n",
    "    'num_leaves': [6,8,12,16],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [42,502], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "lgb = GridSearchCV(estimator, gridParams, verbose=0, cv=cv,n_jobs=n_jobs , scoring = scoring)\n",
    "if(is_pca):\n",
    "    print('asas')\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', lgb)]\n",
    "    lgb = Pipeline(estimators)\n",
    "    lgb = GridSearchCV(lgb, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "# lgb = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter_nt)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = cb.CatBoostClassifier()\n",
    "params = {'depth': st.randint(3, 16),\n",
    "          'learning_rate' : st.uniform(0.05, 0.4),\n",
    "         'l2_leaf_reg': st.randint(0, 10),\n",
    "         'iterations': [1]}\n",
    "\n",
    "cat = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=2, scoring = scoring) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "###### test single model #######\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA ,NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score , roc_curve , auc\n",
    "\n",
    "# nmf = NMF(n_components=5, init='random', random_state=42)\n",
    "\n",
    "# C_OPTIONS = [1, 10, 100, 1000]\n",
    "N_FEATURES_OPTIONS = [20, 25]\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7)],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "#         'classify__C': C_OPTIONS\n",
    "    }\n",
    "]\n",
    "\n",
    "pca = PCA(n_components=25, svd_solver='full',random_state=42)\n",
    "estimators = [('reduce_dim',pca ), ('lgb', xgb)]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "grid\n",
    "\n",
    "# model = cb.CatBoostClassifier(iterations=20, learning_rate=0.25, depth=10, loss_function='Logloss')\n",
    "model = xgb\n",
    "model.fit(x_sm, y_sm)\n",
    "y_pre = model.predict_proba(x_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pre, pos_label=1)\n",
    "f1_1  = auc(fpr, tpr)\n",
    "\n",
    "y_pre_2 = model.predict_prob(x_train)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pre_2, pos_label=1)\n",
    "f1_2  = auc(fpr, tpr)\n",
    "print(f1_1,f1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6392806740121063 0.6480578168082083\n"
     ]
    }
   ],
   "source": [
    "y_pre = model.predict_proba(x_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pre, pos_label=1)\n",
    "f1_1  = auc(fpr, tpr)\n",
    "\n",
    "y_pre_2 = model.predict_proba(x_train)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pre_2, pos_label=1)\n",
    "f1_2  = auc(fpr, tpr)\n",
    "print(f1_1,f1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 17,\n",
       " 'colsample_bytree': 0.9866641258270036,\n",
       " 'gamma': 3.8835260600913024,\n",
       " 'lambda': 4,\n",
       " 'learning_rate': 0.23910223888617654,\n",
       " 'max_depth': 14,\n",
       " 'min_child_weight': 162.51127167841472,\n",
       " 'n_estimators': 18,\n",
       " 'objective': 'binary:logistic',\n",
       " 'rate_drop': 0.9502928648791279,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 42,\n",
       " 'subsample': 0.8944732072319205}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## train #########\n",
    "is_xgb = 1\n",
    "is_log = 1\n",
    "is_knn = 1\n",
    "is_rnf = 1\n",
    "is_ext = 1\n",
    "is_ada = 1\n",
    "is_lgb = 1\n",
    "is_svc = 1\n",
    "is_cat = 1\n",
    "\n",
    "# is_xgb = 0\n",
    "# is_log = 0\n",
    "# is_knn = 0\n",
    "# is_rnf = 0\n",
    "# is_ext = 0\n",
    "# is_ada = 0\n",
    "# is_lgb = 0\n",
    "# is_svc = 0\n",
    "is_cat = 0\n",
    "\n",
    "if(is_xgb == 1):\n",
    "    xgb.fit(x_sm,y_sm)\n",
    "if(is_log == 1):\n",
    "    log.fit(x_sm,y_sm)\n",
    "if(is_knn == 1):\n",
    "    knn.fit(x_sm,y_sm)\n",
    "if(is_rnf == 1):\n",
    "    rnf.fit(x_sm,y_sm)\n",
    "if(is_ext == 1):\n",
    "    ext.fit(x_sm,y_sm)\n",
    "if(is_ada == 1):\n",
    "    ada.fit(x_sm,y_sm)\n",
    "if(is_lgb == 1):\n",
    "    lgb.fit(x_sm,y_sm)\n",
    "if(is_svc == 1):\n",
    "    svc.fit(x_sm,y_sm)\n",
    "if(is_cat == 1):\n",
    "    cat.fit(x_sm,y_sm)\n",
    "\n",
    "########## save #########\n",
    "\n",
    "# joblib.dump(xgb, 'xgb.pkl') \n",
    "# joblib.dump(log, 'log.pkl') \n",
    "# joblib.dump(knn, 'knn.pkl') \n",
    "# joblib.dump(rnf, 'rnf.pkl') \n",
    "# joblib.dump(ext, 'ext.pkl') \n",
    "# joblib.dump(ada, 'ada.pkl') \n",
    "# joblib.dump(lgb, 'svc.pkl') \n",
    "# joblib.dump(svc, 'svc.pkl') \n",
    "# joblib.dump(cat, 'svc.pkl') \n",
    "\n",
    "# vote2 = joblib.load('vote.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_xgb = 0\n",
    "is_log = 0\n",
    "is_knn = 0\n",
    "is_rnf = 0\n",
    "is_ext = 0\n",
    "is_ada = 1\n",
    "is_lgb = 1\n",
    "is_svc = 1\n",
    "############ load ##############\n",
    "# xgb = joblib.load('xgb.pkl')\n",
    "# log = joblib.load('log.pkl') \n",
    "# knn = joblib.load('knn.pkl') \n",
    "# rnf = joblib.load('rnf.pkl') \n",
    "# ext = joblib.load('ext.pkl') \n",
    "# ada = joblib.load('ada.pkl') \n",
    "# lgb = joblib.load('lgb.pkl') \n",
    "# svc = joblib.load('svc.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## params #############\n",
    "if(is_xgb == 1):\n",
    "    best_params1 = xgb.best_params_\n",
    "if(is_log == 1):\n",
    "    best_params2 = log.best_params_\n",
    "if(is_knn == 1):\n",
    "    best_params3 = knn.best_params_\n",
    "if(is_rnf == 1):\n",
    "    best_params4 = rnf.best_params_\n",
    "if(is_ext == 1):\n",
    "    best_params5 = ext.best_params_\n",
    "if(is_ada == 1):\n",
    "    best_params6 = ada.best_params_\n",
    "if(is_lgb == 1):\n",
    "    best_params7 = lgb.best_params_\n",
    "if(is_svc == 1):\n",
    "    best_params8 = svc.best_params_\n",
    "if(is_cat == 1):\n",
    "    best_params9 = cat.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2144, 172) (2144, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5451523498605727 0.6047783607815765 ada\n",
      "0.5088544854791539 0.6103574983270541 lgb\n",
      "0.5518814187580766 0.7022144035413752 svc\n"
     ]
    }
   ],
   "source": [
    "########### evaluate ###########\n",
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score , roc_curve , auc\n",
    "x_val = x_test\n",
    "y_val = y_test\n",
    "def auc_score(model,x_val,y_val,x_train,y_train,name = 'model'):\n",
    "    y_pre = model.predict_proba(x_val)[:,1]\n",
    "    y_pre_2 = model.predict_proba(x_train)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_pre, pos_label=1)\n",
    "    f1  = auc(fpr, tpr)\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, y_pre_2, pos_label=1)\n",
    "    f1_2  = auc(fpr, tpr)\n",
    "    print(f1,f1_2,name)\n",
    "\n",
    "if(is_xgb == 1):\n",
    "    auc_score(xgb,x_val,y_val,x_train,y_train,'xgb')\n",
    "\n",
    "if(is_log == 1):\n",
    "    auc_score(log,x_val,y_val,x_train,y_train,'log')\n",
    "    \n",
    "if(is_knn == 1):\n",
    "    auc_score(knn,x_val,y_val,x_train,y_train,'knn')\n",
    "    \n",
    "if(is_rnf == 1):\n",
    "    auc_score(rnf,x_val,y_val,x_train,y_train,'rnf')\n",
    "    \n",
    "if(is_ext == 1):\n",
    "    auc_score(ext,x_val,y_val,x_train,y_train,'ext')\n",
    "    \n",
    "if(is_ada == 1):\n",
    "    auc_score(ada,x_val,y_val,x_train,y_train,'ada')\n",
    "    \n",
    "if(is_lgb == 1):    \n",
    "    auc_score(lgb,x_val,y_val,x_train,y_train,'lgb')\n",
    "    \n",
    "if(is_svc == 1):\n",
    "    auc_score(svc,x_val,y_val,x_train,y_train,'svc')\n",
    "    \n",
    "if(is_cat == 1):\n",
    "    auc_score(cat,x_val,y_val,x_train,y_train,'cat')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ for vote #############\n",
    "vote_list = []\n",
    "if(is_xgb == 1):\n",
    "    xgb = XGBClassifier(**best_params1)\n",
    "    vote_list.append(('xgb', xgb))\n",
    "if(is_log == 1):\n",
    "    log = LogisticRegression(**best_params2)\n",
    "    vote_list.append(('log', log))\n",
    "if(is_knn == 1):\n",
    "    knn = KNeighborsClassifier(**best_params3)\n",
    "    vote_list.append(('knn', knn))\n",
    "if(is_rnf == 1):\n",
    "    rnf = RandomForestClassifier(**best_params4)\n",
    "    vote_list.append(('ext', ext))\n",
    "if(is_ext == 1):\n",
    "    ext = ExtraTreesClassifier(**best_params5)\n",
    "    vote_list.append(('ext', ext))\n",
    "if(is_ada == 1):\n",
    "    ada = AdaBoostClassifier(**best_params6)\n",
    "    vote_list.append(('ada', ada))\n",
    "if(is_lgb == 1):\n",
    "    lgb = LGBMClassifier(**best_params7)\n",
    "    vote_list.append(('lgb', lgb))\n",
    "if(is_svc == 1):\n",
    "    svc = SVC(**best_params8)\n",
    "    vote_list.append(('svc', svc))\n",
    "if(is_cat == 1):\n",
    "    cat = cb.CatBoostClassifier(**best_params9)\n",
    "    vote_list.append(('cat', cat))\n",
    "    \n",
    "vote = VotingClassifier(estimators=vote_list, voting='soft')\n",
    "vote.fit(x_sm, y_sm)\n",
    "auc_score(vote,x_val,y_val,x_train,y_train,'vote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## feature something ##############\n",
    "a = log.best_estimator_.coef_\n",
    "a = ada.best_estimator_.feature_importances_\n",
    "# print(np.argwhere(a>0.01),np.argwhere(a<-0.01))\n",
    "# print(np.argwhere(a!=0))\n",
    "b = np.argwhere(a!=0)\n",
    "# np.reshape(b,(b.shape[]))\n",
    "b = b.ravel()\n",
    "print(b,np.array([9,10,11,136,148,154,156,158,159]))\n",
    "print(np.argwhere(a>0.01),np.argwhere(a<-0.01))\n",
    "x_sm2 = x_sm[:,np.array(b)]\n",
    "x_sm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# stack ##################\n",
    "xx = x_sm\n",
    "xx_val = x_val\n",
    "xx_train = x_train\n",
    "no_feature = 0\n",
    "\n",
    "def stack_data(model,x_data,result,no_fea = 0):\n",
    "    if(no_fea == 1):\n",
    "        result = model.predict_proba(x_data)[:,1]\n",
    "    else:\n",
    "        re1 = model.predict_proba(x_data)[:,1]\n",
    "        re_1 = re1[:,0]\n",
    "        re_1 = np.reshape(re_1, (re_1.shape[0],1))\n",
    "        result = np.concatenate((result, re_1), axis=1)\n",
    "    return result\n",
    "\n",
    "if(is_xgb == 1):\n",
    "    xx = stack_data(xgb,x_sm,xx)\n",
    "    xx_val = stack_data(xgb,x_val,xx_val)\n",
    "    xx_train = stack_data(xgb,x_train,xx_train)\n",
    "\n",
    "if(is_log == 1):\n",
    "    xx = stack_data(log,x_sm,xx)\n",
    "    xx_val = stack_data(log,x_val,xx_val)\n",
    "    xx_train = stack_data(log,x_train,xx_train)\n",
    "    \n",
    "if(is_knn == 1):\n",
    "    xx = stack_data(knn,x_sm,xx)\n",
    "    xx_val = stack_data(knn,x_val,xx_val)\n",
    "    xx_train = stack_data(knn,x_train,xx_train)\n",
    "\n",
    "if(is_rnf == 1):\n",
    "    xx = stack_data(rnf,x_sm,xx)\n",
    "    xx_val = stack_data(rnf,x_val,xx_val)\n",
    "    xx_train = stack_data(rnf,x_train,xx_train)\n",
    "    \n",
    "if(is_ext == 1):\n",
    "    xx = stack_data(ext,x_sm,xx)\n",
    "    xx_val = stack_data(ext,x_val,xx_val)\n",
    "    xx_train = stack_data(ext,x_train,xx_train)\n",
    "    \n",
    "if(is_ada == 1):\n",
    "    xx = stack_data(ada,x_sm,xx)\n",
    "    xx_val = stack_data(ada,x_val,xx_val)\n",
    "    xx_train = stack_data(ada,x_train,xx_train)\n",
    "    \n",
    "if(is_lgb == 1):\n",
    "    xx = stack_data(lgb,x_sm,xx)\n",
    "    xx_val = stack_data(lgb,x_val,xx_val)\n",
    "    xx_train = stack_data(lgb,xx_train,xx_train)\n",
    "    \n",
    "if(is_svc == 1):\n",
    "    xx = stack_data(svc,x_sm,xx)\n",
    "    xx_val = stack_data(svc,x_val,xx_val)\n",
    "    xx_train = stack_data(svc,x_train,xx_train)\n",
    "    \n",
    "if(is_cat == 1):\n",
    "    xx = stack_data(cat,x_sm,xx)\n",
    "    xx_val = stack_data(cat,x_val,xx_val)\n",
    "    xx_train = stack_data(cat,x_train,xx_train)\n",
    "    \n",
    "#     re1 = cat.predict_proba(x_sm)\n",
    "#     re_1 = re1[:,0]\n",
    "#     re_1 = np.reshape(re_1, (re_1.shape[0],1))\n",
    "#     xx = np.concatenate((xx, re_1), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### for second stack models ###########\n",
    "n_jobs = -1\n",
    "n_iter = 50\n",
    "cv = 5\n",
    "seed=42\n",
    "# n_features=xx.shape[1]\n",
    "n_features= xx.shape[1]\n",
    "\n",
    "estimator = XGBClassifier(nthreads=-1)\n",
    "objective = 'binary:logistic'\n",
    "\n",
    "# Parameter for XGBoost\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 40),\n",
    "    \"max_depth\": st.randint(3, 30),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": st.beta(10, 1),\n",
    "    \"subsample\": st.beta(10, 1),\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'objective': [objective],\n",
    "    'scale_pos_weight': st.randint(0, 2),\n",
    "    \"min_child_weight\": st.expon(0, 50),\n",
    "    \"seed\": [seed],\n",
    "}\n",
    "xgb2 = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "\n",
    "estimator = LogisticRegression()\n",
    "# Parameter for LogisticRegression\n",
    "params = {\n",
    "    \"penalty\": ['l1','l2'],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"tol\": [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "    \"max_iter\": st.randint(50, 100),\n",
    "    'random_state': [seed],\n",
    "} \n",
    "log2 = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "# Parameter for RandomForestClassifier\n",
    "params = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "rnf2 = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "\n",
    "estimator = ExtraTreesClassifier()\n",
    "# Parameter for ExtraTreesClassifier\n",
    "params = {\n",
    "    \"n_estimators\": st.randint(5, 50),\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    \"bootstrap\": [True],\n",
    "    \"oob_score\": [True],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "ext2 = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = 'AUC') \n",
    "\n",
    "estimator = AdaBoostClassifier()\n",
    "# Parameter for AdaBoost\n",
    "params = { \n",
    "    'n_estimators':st.randint(10, 100), \n",
    "    'learning_rate':st.beta(10, 1), \n",
    "    'algorithm':['SAMME', 'SAMME.R'],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "ada2 = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring)\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': [0.005],\n",
    "    'n_estimators': [40],\n",
    "    'num_leaves': [6,8,12,16],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [42,502], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "estimator = LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          n_jobs = 3, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "lgb2 = GridSearchCV(estimator, gridParams, verbose=0, cv=cv,n_jobs=n_jobs, scoring = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############### Xgboost2 ###################\n",
    "############################################\n",
    "import xgboost as xgb\n",
    "dsm2 = xgb.DMatrix(xx, label=y_sm)\n",
    "# dv0 = xgb.DMatrix(x_val0, label=y_val0)\n",
    "dt2 = xgb.DMatrix(xx_test, label=y_test)\n",
    "dtrain = xgb.DMatrix(xx_train, label=y_train)\n",
    "evallist = [(dv0, 'eval'), (dsm2, 'train')]\n",
    "                  \n",
    "dsm = xgb.DMatrix(xx, label=y_sm)                  \n",
    "evallist = [(dt2, 'eval'), (dsm2, 'train')]\n",
    "                  \n",
    "num_round = 10000\n",
    "# binary:logistic\n",
    "param = {'objective': 'binary:logistic',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma': 0.7790711924812199,\n",
    " 'learning_rate': 0.2249426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 150,\n",
    " 'objective': 'binary:logistic',\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': ['auc'],\n",
    " 'lambda': 2,\n",
    " 'alpha': 15,\n",
    "#  'rate_drop':0.5,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921}\n",
    " \n",
    "evals_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train ####\n",
    "bst2 = xgb.train(param, dsm2, num_round, evallist, evals_result=evals_result,early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ###\n",
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score , roc_curve , auc\n",
    "treee = bst.best_ntree_limit\n",
    "# treee = 139\n",
    "y_pre = np.array(bst2.predict(dt2, ntree_limit=treee))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pre, pos_label=1)\n",
    "roc1_1  = auc(fpr, tpr)\n",
    "\n",
    "y_pre_1 = np.array(bst2.predict(dv0, ntree_limit=treee))\n",
    "fpr, tpr, thresholds = roc_curve(y_val0, y_pre_1, pos_label=1)\n",
    "roc1_11  = auc(fpr, tpr)\n",
    "\n",
    "y_pre_2 = np.array(bst2.predict(dtrain2, ntree_limit=treee))\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pre_2, pos_label=1)\n",
    "roc1_2  = auc(fpr, tpr)\n",
    "print(roc1_1,roc1_11,roc1_2)\n",
    "# print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## train scikit #####################\n",
    "xgb2.fit(xx,y_sm)\n",
    "log2.fit(xx,y_sm)\n",
    "rnf2.fit(xx,y_sm)\n",
    "ext2.fit(xx,y_sm)\n",
    "ada2.fit(xx,y_sm)\n",
    "lgb2.fit(xx,y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score , roc_curve , auc\n",
    "\n",
    "def auc_score(model,x_val,y_val,x_train,y_train,name = 'model'):\n",
    "    y_pre = model.predict_proba(x_val)[:,1]\n",
    "    y_pre_2 = model.predict_proba(x_train)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_pre, , pos_label=1)\n",
    "    f1  = auc(fpr, tpr)\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, y_pre_2, , pos_label=1)\n",
    "    f1_2  = auc(fpr, tpr)\n",
    "    print(f1,f1_2,name)\n",
    "\n",
    "auc_score(xgb2,xx_val,y_val,xx_train,y_train,'xgb2')\n",
    "auc_score(log2,xx_val,y_val,xx_train,y_train,'log2')\n",
    "auc_score(rnf2,xx_val,y_val,xx_train,y_train,'rnf2')\n",
    "auc_score(ext2,xx_val,y_val,xx_train,y_train,'ext2')\n",
    "auc_score(ada2,xx_val,y_val,xx_train,y_train,'ada2')\n",
    "auc_score(lgb2,xx_val,y_val,xx_train,y_train,'lgb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## vote 2 #############\n",
    "\n",
    "best_params21 = xgb2.best_params_\n",
    "best_params22 = log2.best_params_\n",
    "best_params23 = rnf2.best_params_\n",
    "best_params24 = ext2.best_params_\n",
    "best_params25 = ada2.best_params_\n",
    "best_params26 = lgb2.best_params_\n",
    "\n",
    "xgb2 = XGBClassifier(**best_params21)\n",
    "log2 = LogisticRegression(**best_params22)\n",
    "rnf2 = RandomForestClassifier(**best_params23)\n",
    "ext2 = ExtraTreesClassifier(**best_params24)\n",
    "ada2 = AdaBoostClassifier(**best_params25)\n",
    "lgb2 = LGBMClassifier(**best_params26)\n",
    "\n",
    "    \n",
    "vote_list = [('xgb2', xgb2), ('log2', log2), ('rnf2', rnf2), ('ext2', ext2), ('ada2', ada2), ('lgb2', lgb2)]\n",
    "vote2 = VotingClassifier(estimators=vote_list, voting='soft')\n",
    "vote2.fit(xx, y_sm)\n",
    "auc_score(vote2,xx_val,y_val,xx_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "dans = xgb.DMatrix(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = np.array(bst.predict(dans, ntree_limit=treee))\n",
    "np.savetxt(\"ans_01.csv\", sv, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(858, 1)"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = bst.predict(dans, ntree_limit=treee)\n",
    "np.argwhere(a>0.5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35      , 0.17234848, 0.26666667, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.15      , 0.05539773, 0.41333333, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.29333333, ..., 1.        , 0.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.525     , 0.19840116, 0.4       , ..., 1.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.05      , 0.01450893, 0.57333333, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.075     , 0.04513889, 0.18666667, ..., 1.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
